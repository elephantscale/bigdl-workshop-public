{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Visualizing Training With Tensorboard\n",
    "\n",
    "### Overview\n",
    "Introducing visual tools\n",
    "\n",
    "### Runtime\n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - About IRIS Dataset\n",
    "\n",
    "This is [Fisher's Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "\n",
    "This dataset contains 150 samples, with 4 dimensions, as follows:\n",
    "\n",
    "1. Petal Length  (c1)\n",
    "2. Petal Width   (c2)\n",
    "3. Sepal Length  (c3)\n",
    "4. Sepal Width   (c4)\n",
    "\n",
    "There are 3 output classes: Setosa, Versicolor, and Virginica.\n",
    "In our output datset, we have simplified this data by making classes simply 1, 2, 3.\n",
    "\n",
    "Here's an example of what the dataset looks like\n",
    "\n",
    "| c1  | c2  | c3  | c4  | label | \n",
    "|-----|-----|-----|-----|-------| \n",
    "| 6.4 | 2.8 | 5.6 | 2.2 | 3     | \n",
    "| 5.0 | 2.3 | 3.3 | 1.0 | 2     | \n",
    "| 4.9 | 2.5 | 4.5 | 1.7 | 3     | \n",
    "| 4.9 | 3.1 | 1.5 | 0.1 | 1     | \n",
    "| 5.7 | 3.8 | 1.7 | 0.3 | 1     | \n",
    "| 4.4 | 3.2 | 1.3 | 0.2 | 1     | \n",
    "| 5.4 | 3.4 | 1.5 | 0.4 | 1     | \n",
    "| 6.9 | 3.1 | 5.1 | 2.3 | 3     | \n",
    "| 6.7 | 3.1 | 4.4 | 1.4 | 2     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "import zoo.version\n",
    "\n",
    "sc = init_nncontext(\"single layer IRIS\")\n",
    "print(\"zoo version : \", zoo.version.__version__)\n",
    "\n",
    "## Spark UI\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Explore Dataset\n",
    "\n",
    "Let's do some basic exploration of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../../data/iris/iris_full.csv\", \\\n",
    "                      header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")\n",
    "print (\"data count \", data.count())\n",
    "data = data.na.drop()\n",
    "print (\"clean data count \", data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupBy('label').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Convert double\n",
    "BigDL needs attributes as double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "# convert everything to double\n",
    "data = data.select([col(c).cast(\"double\") for c in data.columns])\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assembler = VectorAssembler (inputCols=['c1','c2','c3', 'c4'], outputCol='assembled')\n",
    "fv = assembler.transform(data)\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(inputCol=\"assembled\", outputCol=\"scaled\")\n",
    "fv = scaler.fit(fv).transform(fv)\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Convert vectors to array\n",
    "BigDL supports Array\\[\\] type.  Spark ML Vector support coming soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Utils dir to load path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "# print (\"cwd : \", cwd)\n",
    "utils_dir = os.path.abspath(os.path.join(cwd, \"../utils\"))\n",
    "# print(\"utils dir : \", utils_dir)\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "print (\"sys.path: \" , sys.path)\n",
    "\n",
    "my_utils_pyfile = os.path.abspath(os.path.join(utils_dir, 'my_utils.py'))\n",
    "print (\"my_utils file : \", my_utils_pyfile)\n",
    "\n",
    "from my_utils import dense_to_array_udf, sparse_to_array_udf\n",
    "\n",
    "# add file to spark\n",
    "sc.addPyFile(my_utils_pyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert scaled(vector) --> features(array)\n",
    "fv = fv.withColumn('features', dense_to_array_udf('scaled'))\n",
    "\n",
    "fv.printSchema()\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Split Training / Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split 70% training, 30% validation\n",
    "(training, validation) = fv.randomSplit([0.7,0.3])\n",
    "print(\"training set count \", training.count())\n",
    "print(\"validation set count \", validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Setup Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Designing the network\n",
    "Here's a picture of a simple neural network, like what we have in this example:\n",
    "\n",
    "<img src=\"../../media/feed-forward-1-skitch.png\">\n",
    "\n",
    "\n",
    "As you can see, we have a total of 3 layers:\n",
    "\n",
    "1. Input layer (sized as number of features -- in this case 4)\n",
    "2. Hidden Layer (size we have to specify as part of the model).\n",
    "3. Output Layer (Number of output classes we are trying to classify -- in this case 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Sizing hidden layers\n",
    "\n",
    "Sizing hidden layers can be a challenge. The best way to figure this out is to do it empirically. However, we may need a \"rule of thumb\" to start. Here is a good rule of thumb:\n",
    "\n",
    "First Hidden Layer:\n",
    "\n",
    "```\n",
    "n_hidden_1 = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "```\n",
    "\n",
    "Second Hidden Layer: (if needed)\n",
    "\n",
    "```\n",
    "n_hidden_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "```\n",
    "\n",
    "In this case, we have a VERY simple dataset. We may not need two hidden layers. Let's start with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "import numpy as np\n",
    "\n",
    "n_input = 4  # c1-4\n",
    "n_classes = 3  # outcome 1/2/3\n",
    "\n",
    "n_hidden_guess = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "print(\"Hidden layer 1 (Guess) : \" + str(n_hidden_guess))\n",
    "\n",
    "n_hidden_guess_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "print(\"Hidden layer 2 (Guess) : \" + str(n_hidden_guess_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Setup BigDL Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "# batch size should be multiple of number of cores.\n",
    "# So powers of two is a good bet\n",
    "batch_size = 32\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 4  # c1-3\n",
    "n_classes = 3  # outcome 1/2/3\n",
    "n_hidden_1 = 3  # from the above guess\n",
    "# n_hidden_2 = 3  # 2nd layer number of neurons (from guess above)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - setup BigDL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import Sequential, Linear, LogSoftMax\n",
    "from bigdl.nn.criterion import ClassNLLCriterion\n",
    "from zoo.pipeline.nnframes import  NNClassifier\n",
    "from bigdl.optim.optimizer import Adam, SGD, Adagrad\n",
    "\n",
    "nn = Sequential()\\\n",
    "     .add(Linear(n_input, n_hidden_1))\\\n",
    "     .add(Linear(n_hidden_1, n_classes))\\\n",
    "     .add(LogSoftMax())\n",
    "\n",
    "estimator = NNClassifier(nn, ClassNLLCriterion(), [n_input])\n",
    "estimator.setMaxEpoch(training_epochs)\\\n",
    "            .setBatchSize(batch_size)\\\n",
    "            .setLearningRate(learning_rate)\n",
    "estimator.setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "# optimizer method, default is SGD\n",
    "estimator.setOptimMethod(Adam())\n",
    "\n",
    "print (\"nn \\n\", nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Setup tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Cleanup tensor logs directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "# import datetime as dt\n",
    "\n",
    "\n",
    "tensorboard_dir=os.environ.get('TENSORBOARD_DIR', '/tmp/tensorboard-logs')\n",
    "print(\"TENSORBOARD_DIR : \", tensorboard_dir)\n",
    "\n",
    "## TODO : give an app name\n",
    "app_name='???' #+dt.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "base_path = os.path.abspath(os.path.join(tensorboard_dir, app_name))\n",
    "\n",
    "# clean old logs\n",
    "try:\n",
    "    print (\"Cleaning : \", base_path)\n",
    "    shutil.rmtree(base_path)\n",
    "#     shutil.rmtree('/private' + base_path)  # On Mac\n",
    "except OSError:\n",
    "    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Setup validation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.optim.optimizer import EveryEpoch, Top1Accuracy, TrainSummary, SeveralIteration, ValidationSummary\n",
    "\n",
    "estimator.setValidation(EveryEpoch(), \\\n",
    "                        validation, \\\n",
    "                        [Top1Accuracy()], \\\n",
    "                        batch_size)\n",
    "\n",
    "## TODO : create a trining summary, \n",
    "##   hint : log_dir=tensorboard_dir\n",
    "##          app_name=app_name\n",
    "train_summary = TrainSummary(log_dir=???, app_name=???)\n",
    "train_summary.set_summary_trigger(\"Parameters\", SeveralIteration(50))\n",
    "\n",
    "## TODO : create a validation summary\n",
    "##   hint : log_dir=tensorboard_dir\n",
    "##          app_name=app_name\n",
    "val_summary = ValidationSummary(log_dir=???, app_name=???)\n",
    "\n",
    "log_path = os.path.abspath(os.path.join(tensorboard_dir, app_name))\n",
    "print(\"saving logs to \",log_path)\n",
    "\n",
    "## TODO : set training summary (train_summary)\n",
    "estimator.setTrainSummary(???)\n",
    "\n",
    "## TODO : set validation summary (val_summary)\n",
    "estimator.setValidationSummary(???)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## training\n",
    "print (\"starting training...\")\n",
    "model = estimator.fit(training)\n",
    "print(\"training finished.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9.2 - Visualize Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "loss = np.array(train_summary.read_scalar(\"Loss\"))\n",
    "top1 = np.array(val_summary.read_scalar(\"Top1Accuracy\"))\n",
    "\n",
    "plt.figure(figsize = (12,12))\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(loss[:,0],loss[:,1],label='loss')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.grid(True)\n",
    "plt.title(\"loss\")\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(top1[:,0],top1[:,1],label='top1')\n",
    "plt.xlim(0,loss.shape[0]+10)\n",
    "plt.title(\"top1 accuracy\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Predict\n",
    "We use 'test' dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "predictions = model.transform(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 - Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"matching predictions \", predictions.filter(\"prediction == label\").count())\n",
    "print (\"missed predictions \", predictions.filter(\"prediction != label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2 - Accuracy, Precision, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auPRC = evaluator.evaluate(predictions)\n",
    "print(\"Area under precision-recall curve = \" , auPRC)\n",
    "    \n",
    "recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictions)\n",
    "print(\"recall = \" , recall)\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision = \", precision)\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").\\\n",
    "            evaluate(predictions)\n",
    "print(\"accuracy = \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# we use Spark to calculate confusion matrix as the prediction set can be rather large\n",
    "cm = predictions.groupBy('label').pivot('prediction', [1,2,3]).count().na.fill(0).orderBy('label')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "# print(cm_pd)\n",
    "cm_pd = cm_pd.set_index('label')  # make 'label' as index\n",
    "# print(cm_pd)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(cm_pd, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
