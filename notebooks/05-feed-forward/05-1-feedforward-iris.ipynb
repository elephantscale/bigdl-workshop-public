{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Feedforward Neural Network on the Iris Dataset\n",
    "\n",
    "### Overview\n",
    "Classify IRIS dataset using Feed Forward Network\n",
    "\n",
    "### Runtime\n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - About IRIS Dataset\n",
    "\n",
    "This is [Fisher's Iris dataset](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "\n",
    "This dataset contains 150 samples, with 4 dimensions, as follows:\n",
    "\n",
    "1. Petal Length  (c1)\n",
    "2. Petal Width   (c2)\n",
    "3. Sepal Length  (c3)\n",
    "4. Sepal Width   (c4)\n",
    "\n",
    "There are 3 output classes: Setosa, Versicolor, and Virginica.\n",
    "In our output datset, we have simplified this data by making classes simply 1, 2, 3.\n",
    "\n",
    "Here's an example of what the dataset looks like\n",
    "\n",
    "| c1  | c2  | c3  | c4  | label | \n",
    "|-----|-----|-----|-----|-------| \n",
    "| 6.4 | 2.8 | 5.6 | 2.2 | 3     | \n",
    "| 5.0 | 2.3 | 3.3 | 1.0 | 2     | \n",
    "| 4.9 | 2.5 | 4.5 | 1.7 | 3     | \n",
    "| 4.9 | 3.1 | 1.5 | 0.1 | 1     | \n",
    "| 5.7 | 3.8 | 1.7 | 0.3 | 1     | \n",
    "| 4.4 | 3.2 | 1.3 | 0.2 | 1     | \n",
    "| 5.4 | 3.4 | 1.5 | 0.4 | 1     | \n",
    "| 6.9 | 3.1 | 5.1 | 2.3 | 3     | \n",
    "| 6.7 | 3.1 | 4.4 | 1.4 | 2     | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "import zoo.version\n",
    "\n",
    "## TODO : use 'init_nncontext (\"your app name\")' to initialize the app\n",
    "sc = ???(\"???\")\n",
    "print(\"zoo version : \", zoo.version.__version__)\n",
    "\n",
    "## Spark UI\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Explore Dataset\n",
    "\n",
    "Let's do some basic exploration of dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../../data/iris/iris_full.csv\", \\\n",
    "                      header=True, inferSchema=\"true\", mode=\"DROPMALFORMED\")\n",
    "print (\"data count \", data.count())\n",
    "data = data.na.drop()\n",
    "print (\"clean data count \", data.count())\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Basic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Spark's describe function is pretty powerful\n",
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 -  See how data is distributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : see data distributed\n",
    "## Hint : groupBy('label')\n",
    "\n",
    "data.groupBy(\"???\").count().show()\n",
    "\n",
    "# we see the data is pretty evenly distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - basic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class_count = data.groupBy(\"label\").count().orderBy('label').toPandas()\n",
    "class_count = class_count.set_index('label')\n",
    "class_count.plot(kind='bar', rot=0)\n",
    "plt.xlabel(\"Label\")\n",
    "plt.ylabel(\"Frequency\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Convert double\n",
    "BigDL needs attributes as double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "## TODO : convert everything to double\n",
    "## Hint : cast all columns to 'double'\n",
    "data = data.select([col(c).cast(\"???\") for c in data.columns])\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Assembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : assemble a feature vector\n",
    "## Hint : inputCols = ['c1', 'c2', 'c3', 'c4']\n",
    "assembler = VectorAssembler (inputCols=['c1','c2','???', '???'], outputCol='assembled')\n",
    "fv = assembler.transform(data)\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Scale Features\n",
    "It is important to scale features, so their values are normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "## TODO : Use 'StandardScaler' to scale the features\n",
    "## Hint : inputCol='assembled',  outputCol='scaled'\n",
    "scaler = ???(inputCol=\"???\", outputCol=\"???\")\n",
    "fv = scaler.fit(fv).transform(fv)\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Convert vectors to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Utils dir to load path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "# print (\"cwd : \", cwd)\n",
    "utils_dir = os.path.abspath(os.path.join(cwd, \"../utils\"))\n",
    "# print(\"utils dir : \", utils_dir)\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "print (\"sys.path: \" , sys.path)\n",
    "\n",
    "my_utils_pyfile = os.path.abspath(os.path.join(utils_dir, 'my_utils.py'))\n",
    "print (\"my_utils file : \", my_utils_pyfile)\n",
    "\n",
    "from my_utils import dense_to_array_udf, sparse_to_array_udf\n",
    "\n",
    "# add file to spark\n",
    "sc.addPyFile(my_utils_pyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert scaled(vector) --> features(array)\n",
    "fv = fv.withColumn('features', dense_to_array_udf('???'))\n",
    "\n",
    "fv.printSchema()\n",
    "fv.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Split Training / Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : split 70% training, 30% validation\n",
    "## Hint : 70% = 0.7 ,  30% = 0.3\n",
    "(training, validation) = fv.randomSplit([???, ???])\n",
    "\n",
    "## TODO : print out the record count in training and validation sets\n",
    "## Hint : 'count'\n",
    "print(\"training set count \", training.???())\n",
    "print(\"validation set count \", validation.???())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Setup Neural Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Designing the network\n",
    "Here's a picture of a simple neural network, like what we have in this example:\n",
    "\n",
    "<img src=\"../../media/feed-forward-1-skitch.png\">\n",
    "\n",
    "\n",
    "As you can see, we have a total of 3 layers:\n",
    "\n",
    "1. Input layer (sized as number of features -- in this case 4)\n",
    "2. Hidden Layer (size we have to specify as part of the model).\n",
    "3. Output Layer (Number of output classes we are trying to classify -- in this case 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Sizing hidden layers\n",
    "\n",
    "Sizing hidden layers can be a challenge. The best way to figure this out is to do it empirically. However, we may need a \"rule of thumb\" to start. Here is a good rule of thumb:\n",
    "\n",
    "First Hidden Layer:\n",
    "\n",
    "```\n",
    "n_hidden_1 = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "```\n",
    "\n",
    "Second Hidden Layer: (if needed)\n",
    "\n",
    "```\n",
    "n_hidden_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "```\n",
    "\n",
    "In this case, we have a VERY simple dataset. We may not need two hidden layers. Let's start with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "import numpy as np\n",
    "\n",
    "n_input = 4  # c1-4\n",
    "n_classes = 3  # outcome 1/2/3\n",
    "\n",
    "n_hidden_guess = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "print(\"Hidden layer 1 (Guess) : \" + str(n_hidden_guess))\n",
    "\n",
    "n_hidden_guess_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "print(\"Hidden layer 2 (Guess) : \" + str(n_hidden_guess_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Setup BigDL Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "training_epochs = 100\n",
    "# batch size should be multiple of number of cores.\n",
    "# So powers of two is a good bet\n",
    "batch_size = 32\n",
    "\n",
    "# Network Parameters\n",
    "## TODO : define input / output numbers\n",
    "## Hint : how many input features are we feeding?\n",
    "## Hint : How many output classes?\n",
    "n_input = ???  # c1-3\n",
    "n_classes = ???  # outcome 1/2/3\n",
    "n_hidden_1 = ???  # from the above guess\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - setup BigDL network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import Sequential, Linear, LogSoftMax\n",
    "from bigdl.nn.criterion import ClassNLLCriterion\n",
    "from zoo.pipeline.nnframes import  NNClassifier\n",
    "from bigdl.optim.optimizer import Adam, SGD, Adagrad\n",
    "\n",
    "## TODO : setup network\n",
    "nn = Sequential()\\\n",
    "     .add(Linear(???, ???))\\  # hint : input --> hidden1\n",
    "     .add(Linear(???, ???))\\  # hint : hidden1 --> output\n",
    "     .add(???())     #  hint : LogSoftMax\n",
    "\n",
    "## TODO : use a 'ClassNLLCriterion'\n",
    "criterion = ???()\n",
    "\n",
    "## TODO : create NNClassifier with parameters : network, criterion, and input_size\n",
    "estimator = ???(??, ???, [???])\n",
    "\n",
    "## TODO : set training parameters\n",
    "estimator.setMaxEpoch(???)\\\n",
    "            .setBatchSize(???)\\\n",
    "            .setLearningRate(???)\n",
    "\n",
    "## TODO : set featuresCol='features',  labelCol='label'\n",
    "estimator.setLabelCol(\"???\").setFeaturesCol(\"???\")\n",
    "\n",
    "# TODO : set an optimizer method 'Adam()', default is SGD\n",
    "estimator.setOptimMethod(???())\n",
    "\n",
    "print (\"nn \\n\", nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Train the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## training\n",
    "print (\"starting training...\")\n",
    "## TODO : do training on 'training' dataset\n",
    "## Hint : call 'fit' function with 'training' parameter\n",
    "model = estimator.???(???)\n",
    "print(\"training finished.\\n\")\n",
    "\n",
    "## TODO : note the time it took for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 - Predict\n",
    "We use 'test' dataset for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO : do predictions\n",
    "## Hint : call 'transform' function, pass in 'validation' dataset\n",
    "predictions = model.???(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()\n",
    "predictions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9.1 - Basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"matching predictions \", predictions.filter(\"prediction == label\").count())\n",
    "\n",
    "## TODO  : print missed prediction count\n",
    "## Hint : adjust the condition for 'filter' function\n",
    "print (\"missed predictions \", predictions.filter(\"prediction ??? label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Accuracy, Precision, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "auPRC = evaluator.evaluate(predictions)\n",
    "print(\"Area under precision-recall curve = \" , auPRC)\n",
    "    \n",
    "recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictions)\n",
    "print(\"recall = \" , recall)\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision = \", precision)\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").\\\n",
    "            evaluate(predictions)\n",
    "print(\"accuracy = \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# we use Spark to calculate confusion matrix as the prediction set can be rather large\n",
    "cm = predictions.groupBy('label').pivot('prediction', [1,2,3]).count().na.fill(0).orderBy('label')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Plot\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "# print(cm_pd)\n",
    "cm_pd = cm_pd.set_index('label')  # make 'label' as index\n",
    "# print(cm_pd)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(cm_pd, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Experiment\n",
    "Do a few runs (`Cell --> Run All`) and try the following\n",
    "- change hidden layer sizing (3,4,5)\n",
    "- change learning rate (0.0001 --> 0.01)\n",
    "\n",
    "And observe the accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
