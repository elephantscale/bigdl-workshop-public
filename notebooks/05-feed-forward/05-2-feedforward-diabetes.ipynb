{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Diabetes\n",
    "\n",
    "### Overview\n",
    "Analyze some diabetes data\n",
    "\n",
    "### Runtime\n",
    "30 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 -About Data\n",
    "\n",
    "[About data](https://www.kaggle.com/uciml/pima-indians-diabetes-database)\n",
    "\n",
    "This is a classification dataset, based on inputs (`a,b,c,d,e,f,g,h`) we predict the `outcome`\n",
    "\n",
    "Sample Data:\n",
    "\n",
    "```\n",
    "a,b,c,d,e,f,g,h,outcome\n",
    "6,148,72,35,0,33.6,0.627,50,1\n",
    "1,85,66,29,0,26.6,0.351,31,0\n",
    "8,183,64,0,0,23.3,0.672,32,1\n",
    "1,89,66,23,94,28.1,0.167,21,0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "import zoo.version\n",
    "\n",
    "## TODO : use 'init_nncontext (\"your app name\")' to initialize the app\n",
    "sc = ???(\"???\")\n",
    "print(\"zoo version : \", zoo.version.__version__)\n",
    "\n",
    "## Spark UI\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Explore Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv(\"../../data/diabetes/pima-indians-diabetes-data.csv\", \\\n",
    "                      header=True, inferSchema=True)\n",
    "print(\"record count \", data.count())\n",
    "data = data.na.drop()\n",
    "print (\"clean data count \", data.count())\n",
    "data.printSchema()\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Basic Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().toPandas().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : see data distributed\n",
    "## Hint : groupBy('outcome')\n",
    "\n",
    "data.groupBy(\"???\").count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic frequency graph\n",
    "\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "a = data.groupBy(\"outcome\").count().toPandas()\n",
    "print(a)\n",
    "a = a.set_index('outcome')\n",
    "a.plot(kind='bar', rot=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create Feature Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - No zeroes in Target Label column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BigdL doesn't like 0 (zero) in label column\n",
    "# so I am going to add +1 to label\n",
    "\n",
    "##TODO : create another column 'outcome2'\n",
    "## We add 1 to 'outcome' column\n",
    "data = data.withColumn(\"???\", data['outcome']+1)\n",
    "\n",
    "## TODO : group by 'outcome2'\n",
    "data.groupBy(\"???\").count().show()\n",
    "\n",
    "\n",
    "data.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Convert to Double\n",
    "BigDL likes all numbers in Double"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "## TODO : convert everything to double\n",
    "## Hint : cast all columns to 'double'\n",
    "data = data.select([col(c).cast(\"???\") for c in data.columns])\n",
    "data.printSchema()\n",
    "data.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "## TODO : create a feature vector from columns : 'a','b','c','d','e','f','g', 'h'\n",
    "assembler = VectorAssembler (inputCols=['a', '?', '?'], outputCol='assembled')\n",
    "fv = assembler.transform(data)\n",
    "\n",
    "fv = fv.withColumn ('label', fv['outcome2'])\n",
    "fv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "## TODO : scale 'assembled' --> 'features'\n",
    "scaler = StandardScaler (inputCol=\"???\", outputCol=\"???\")\n",
    "\n",
    "fv = scaler.fit(fv).transform(fv)\n",
    "fv.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Convert label & feature to arrays\n",
    "BigDL supports Array\\[\\] type.  Spark ML Vector support coming soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Utils dir to load path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "# print (\"cwd : \", cwd)\n",
    "utils_dir = os.path.abspath(os.path.join(cwd, \"../utils\"))\n",
    "# print(\"utils dir : \", utils_dir)\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "print (\"sys.path: \" , sys.path)\n",
    "\n",
    "my_utils_pyfile = os.path.abspath(os.path.join(utils_dir, 'my_utils.py'))\n",
    "print (\"my_utils file : \", my_utils_pyfile)\n",
    "\n",
    "from my_utils import dense_to_array_udf, sparse_to_array_udf\n",
    "\n",
    "# add file to spark\n",
    "sc.addPyFile(my_utils_pyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert scaled(vector) --> features(array)\n",
    "fv = fv.withColumn('features', dense_to_array_udf('scaled'))\n",
    "\n",
    "fv.printSchema()\n",
    "fv.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Split training / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : split the data 70%, 30 %\n",
    "\n",
    "(training, validation) = fv.randomSplit([???, ???])\n",
    "print(\"training set count \", training.count())\n",
    "print(\"validation set count \", validation.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Design Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 - Designing the network\n",
    "Here's a picture of a simple neural network, like what we have in this example:\n",
    "\n",
    "<img src=\"../../media/diabetes-hidden-layer-skitch.png\">\n",
    "\n",
    "\n",
    "As you can see, we have a total of 3 layers:\n",
    "\n",
    "1. Input layer (sized as number of features -- in this case 8 : 'a' -- 'h')\n",
    "2. Hidden Layer (size we have to specify as part of the model).\n",
    "3. Output Layer (Number of output classes we are trying to classify -- in this case 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 - Sizing hidden layers\n",
    "\n",
    "Sizing hidden layers can be a challenge. The best way to figure this out is to do it empirically. However, we may need a \"rule of thumb\" to start. Here is a good rule of thumb:\n",
    "\n",
    "First Hidden Layer:\n",
    "\n",
    "```\n",
    "n_hidden_1 = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "```\n",
    "\n",
    "Second Hidden Layer: (if needed)\n",
    "\n",
    "```\n",
    "n_hidden_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "```\n",
    "\n",
    "In this case, we have a VERY simple dataset. We may not need two hidden layers. Let's start with one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of hidden layers\n",
    "import numpy as np\n",
    "\n",
    "## TODO : define input / output numbers\n",
    "## Hint : how many input features are we feeding?\n",
    "## Hint : How many output classes?\n",
    "n_input = ???  # # a -h \n",
    "n_classes = ???  # outcome 1/2\n",
    "\n",
    "n_hidden_guess = np.sqrt(np.sqrt((n_classes + 2) * n_input) + 2 * np.sqrt(n_input /(n_classes+2.)))\n",
    "print(\"Hidden layer 1 (Guess) : \" + str(n_hidden_guess))\n",
    "\n",
    "n_hidden_guess_2 = n_classes * np.sqrt(n_input / (n_classes + 2.))\n",
    "print(\"Hidden layer 2 (Guess) : \" + str(n_hidden_guess_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step  7 -  Create the Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "training_epochs = 100\n",
    "\n",
    "# batch size should be multiple of number of cores.\n",
    "# So powers of two is a good bet, start with 32\n",
    "batch_size = ???\n",
    "\n",
    "\n",
    "n_hidden_1 = 5 # 1st layer number of neurons\n",
    "n_hidden_2 = 3  # 2nd layer number of neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import Sequential, Linear, LogSoftMax\n",
    "from bigdl.nn.criterion import ClassNLLCriterion\n",
    "from zoo.pipeline.nnframes import  NNClassifier\n",
    "from bigdl.optim.optimizer import Adam, SGD, Adagrad\n",
    "\n",
    "## two layers =  input [8] + output [2]\n",
    "# nn = Sequential().add(Linear(n_input, n_classes)).add(LogSoftMax())\n",
    "\n",
    "## TODO : setup a network\n",
    "## 3 layers = input [8] +  hidden1  + output [2]\n",
    "nn = Sequential().\\\n",
    "       add(Linear(???, ???)).\\   # Hint : n_input --> n_hidden_1\n",
    "       add(Linear(???, ???)).\\   # Hint : n_hidden_1 --> n_output\n",
    "       add(???())                # Hint : LogSoftMax\n",
    "\n",
    "## 4 layers = input [8] +  hidden1   +  hidden2 + output [2]\n",
    "# nn = Sequential().add(Linear(???, ???)).\\   # n_input --> n_hidden1\n",
    "#                   add(Linear(???, ???)).\\   # n_hidden1 --> n_hidden2\n",
    "#                   add(Linear(???, ???)).\\   # n_hidden2 --> n_output \n",
    "#                   add(???())              # LogSoftMax\n",
    "\n",
    "\n",
    "criterion = ClassNLLCriterion()\n",
    "\n",
    "## TODO : Create 'NNClassifier' with 'network', 'criterion' and 'n_input'\n",
    "estimator = ???(???, ???, [???])\n",
    "\n",
    "## TODO : set other network parameters\n",
    "estimator.setMaxEpoch(???)\\\n",
    "            .setBatchSize(???)\\\n",
    "            .setLearningRate(???)\n",
    "\n",
    "estimator.setLabelCol(\"label\").setFeaturesCol(\"features\")\n",
    "\n",
    "## TODO :  optimizer method to 'Adam()', default is SGD\n",
    "estimator.setOptimMethod(???())\n",
    "\n",
    "print (\"nn \\n\", nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 - Train / Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 - Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "## training\n",
    "print (\"starting training...\")\n",
    "## TODO : train using 'fit' method , pass in 'training' data\n",
    "model = estimator.???(???)\n",
    "print(\"initial model training finished.\")\n",
    "\n",
    "# TODO : notice the time it took for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2 -  Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "## TODO : predict using 'validation'\n",
    "predictions = model.transform(???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.groupBy(\"prediction\").count().show()\n",
    "predictions.sample(False, 0.1).limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 - Evalauating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 - Basic Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"matching predictions \", predictions.filter(\"prediction == label\").count())\n",
    "print (\"missed predictions \", predictions.filter(\"prediction != label\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2 - Accuracy, Precision, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"prediction\")\n",
    "auPRC = evaluator.evaluate(predictions)\n",
    "print(\"Area under precision-recall curve = \" , auPRC)\n",
    "    \n",
    "recall = MulticlassClassificationEvaluator(metricName=\"weightedRecall\").evaluate(predictions)\n",
    "print(\"recall = \" , recall)\n",
    "\n",
    "precision = MulticlassClassificationEvaluator(metricName=\"weightedPrecision\").evaluate(predictions)\n",
    "print(\"Precision = \", precision)\n",
    "\n",
    "accuracy = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\").\\\n",
    "            evaluate(predictions)\n",
    "print(\"accuracy = \",  accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3 - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "# we use Spark to calculate confusion matrix as the prediction set can be rather large\n",
    "cm = predictions.groupBy('label').pivot('prediction', [1,2]).count().na.fill(0).orderBy('label')\n",
    "cm.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "cm_pd = cm.toPandas()\n",
    "# print(cm_pd)\n",
    "cm_pd = cm_pd.set_index('label')  # make 'label' as index\n",
    "# print(cm_pd)\n",
    "\n",
    "plt.figure(figsize = (10,8))\n",
    "sn.heatmap(cm_pd, annot=True,fmt='d');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10 - Experiment\n",
    "Try the following :\n",
    "- increase number of hidden layers (3 --> 4 --> 5)\n",
    "- you can also adjust the number of neurons on each \n",
    "\n",
    "See if you can improve the accuracy and confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
