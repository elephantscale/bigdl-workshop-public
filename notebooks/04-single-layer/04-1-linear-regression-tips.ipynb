{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "**Overview**   \n",
    "We will implement a simple linear regression in BigDL\n",
    "\n",
    "**Run time**  \n",
    "15 mins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "import zoo.version\n",
    "\n",
    "## TODO : use 'init_nncontext'\n",
    "sc = ???(\"Linear Regression\")\n",
    "print(\"zoo version : \", zoo.version.__version__)\n",
    "\n",
    "## Spark UI\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Create some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "bill = np.array([50.00, 30.00, 60.00, 40.00, 65.00, 20.00, 10.00, 15.00, 25.00, 35.00])\n",
    "tip = np.array( [12.00, 7.00, 13.00, 8.00, 15.00, 5.00, 2.00, 2.00, 3.00, 4.00])\n",
    "\n",
    "tip_data = pd.DataFrame({'bill' : bill,\n",
    "              'tip' : tip\n",
    "             })\n",
    "\n",
    "tip_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Basic plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(bill, tip)\n",
    "plt.ylabel('tip')\n",
    "plt.xlabel('bill')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Create a Spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Create a Spark Dataframe\n",
    "## Hint : pass in 'tips_data'\n",
    "spark_tips = spark.createDataFrame(???)\n",
    "\n",
    "spark_tips.printSchema()\n",
    "spark_tips.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Create a Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "## TODO : Create feature vector for input 'bill'\n",
    "## Hint : inputCols = ['bill']\n",
    "assembler1 = VectorAssembler(inputCols=[???], outputCol=\"assembled\")\n",
    "feature_vector = assembler1.transform(spark_tips)\n",
    "\n",
    "## TODO : create a feature vector for tip column\n",
    "## Hint : inputCols = ['tip']\n",
    "assembler2 = VectorAssembler(inputCols=[\"???\"], outputCol=\"outcome\")\n",
    "feature_vector = assembler2.transform(feature_vector)\n",
    "\n",
    "feature_vector.printSchema()\n",
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 - Create Array type\n",
    "NNClassifier currently supports Array type.  \n",
    "(Spark ML Vector support coming soon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Utils dir to load path\n",
    "\n",
    "import os\n",
    "import sys\n",
    "cwd = os.getcwd()\n",
    "# print (\"cwd : \", cwd)\n",
    "utils_dir = os.path.abspath(os.path.join(cwd, \"../utils\"))\n",
    "# print(\"utils dir : \", utils_dir)\n",
    "if utils_dir not in sys.path:\n",
    "    sys.path.append(utils_dir)\n",
    "print (\"sys.path: \" , sys.path)\n",
    "\n",
    "my_utils_pyfile = os.path.abspath(os.path.join(utils_dir, 'my_utils.py'))\n",
    "print (\"my_utils file : \", my_utils_pyfile)\n",
    "\n",
    "from my_utils import dense_to_array_udf, sparse_to_array_udf\n",
    "\n",
    "# add file to spark\n",
    "sc.addPyFile(my_utils_pyfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : convert 'assembled' into 'features'\n",
    "feature_vector = feature_vector.withColumn('features', dense_to_array_udf('???'))\n",
    "\n",
    "## TODO : convert 'outcome' into 'label'\n",
    "feature_vector = feature_vector.withColumn('label', dense_to_array_udf('???'))\n",
    "\n",
    "feature_vector.printSchema()\n",
    "feature_vector.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 - Create Linear Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : set network parameters\n",
    "\n",
    "## Hint : set both 'input_size' and 'output_size'  to 1\n",
    "\n",
    "## batch size : set to power of 2, but it can not be higher than the total number of inputs\n",
    "## start with 4\n",
    "\n",
    "## max_epoch : set to 100\n",
    "\n",
    "## Learning rate : set to 0.001\n",
    "\n",
    "input_size = ???\n",
    "output_size = ???\n",
    "batch_size = ??\n",
    "max_epochs = ???\n",
    "\n",
    "learning_rate = ???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - Setup network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import Linear, Sequential\n",
    "from bigdl.nn.criterion import MSECriterion\n",
    "from zoo.pipeline.nnframes import  NNEstimator\n",
    "from zoo.pipeline.nnframes.nn_classifier import SeqToTensor, ArrayToTensor\n",
    "\n",
    "\n",
    "\n",
    "## TODO : create a simple linear network, just input to output\n",
    "## Hint : input_size=input_size,   output_size=output_size (they are both 1)\n",
    "nn = Sequential()\\\n",
    "    .add(Linear(input_size=???, output_size=???))\n",
    "\n",
    "# MSE : https://en.wikipedia.org/wiki/Mean_squared_error\n",
    "## TODO : use 'MSECriterion'\n",
    "criterion = ???()\n",
    "\n",
    "## TODO : Create NNEstimator with correct parameters\n",
    "## Hint : \n",
    "##     first param : nn (network)\n",
    "##     second param : criterion\n",
    "##     third param : input size \n",
    "##     fourth param : output size \n",
    "\n",
    "estimator = ???(???, ???, SeqToTensor([???]), ArrayToTensor([???]))\n",
    "\n",
    "#TODO : set other params\n",
    "estimator.setBatchSize(???)\\\n",
    "         .setMaxEpoch(???)\\\n",
    "         .setLearningRate(???)\n",
    "\n",
    "print(\"nn:\\n\", nn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 : Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print(\"training started...\")\n",
    "## TODO : train using 'training' data\n",
    "## Hint : pass in 'feature_vector'\n",
    "model = estimator.???(???)\n",
    "print(\"training done\\n\")\n",
    "\n",
    "## TODO : Notice the time taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 : Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : Create predictions by using 'transform' method\n",
    "##        Pass in 'feature_vector' as parameter\n",
    "\n",
    "predictions = model.???(???)\n",
    "predictions.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 : Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 - Calculate residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = predictions.withColumn('residual', predictions['label'][0] - predictions['prediction'][0])\n",
    "residuals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 - SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import sum\n",
    "\n",
    "residual_squared = residuals.withColumn(\"residual_squared\", residuals['residual'] * residuals['residual'])\n",
    "residual_squared.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = residual_squared.agg(sum('residual_squared')).first()[0]\n",
    "print (\"SSE = \" , sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
