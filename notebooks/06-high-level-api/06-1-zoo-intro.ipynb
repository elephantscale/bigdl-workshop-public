{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab : Zoo High Level APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1- Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zoo.common.nncontext import init_nncontext\n",
    "\n",
    "## TODO : use 'init_nncontext' to initialize Spark context.\n",
    "##   You can pass a string as program argument\n",
    "sc = ???(\"???\")\n",
    "\n",
    "print('Spark UI running on http://localhost:' + sc.uiWebUrl.split(':')[2])\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Regression Example\n",
    "\n",
    "Here we will see a simple regression example using Analytics zoo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, ArrayType, DoubleType\n",
    "\n",
    "data = sc.parallelize([\n",
    "    ((2.0, 1.0), (1.0, 2.0)),\n",
    "    ((1.0, 2.0), (2.0, 1.0)),\n",
    "    ((2.0, 1.0), (1.0, 2.0)),\n",
    "    ((1.0, 2.0), (2.0, 1.0))])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"features\", ArrayType(DoubleType(), False), False),\n",
    "    StructField(\"label\", ArrayType(DoubleType(), False), False)])\n",
    "df = sqlContext.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bigdl.nn.layer import Sequential, Linear\n",
    "from bigdl.nn.criterion import MSECriterion\n",
    "from zoo.pipeline.nnframes.nn_classifier import NNEstimator\n",
    "from zoo.pipeline.nnframes.nn_classifier import SeqToTensor, ArrayToTensor\n",
    "\n",
    "model = Sequential().add(Linear(2, 2))\n",
    "criterion = MSECriterion()\n",
    "\n",
    "## TODO : \n",
    "##    - set batch size to 4\n",
    "##    - set learning rate to 0.2\n",
    "##    - set epoch to 20\n",
    "estimator = NNEstimator(model, criterion, SeqToTensor([2]), ArrayToTensor([2]))\\\n",
    "    .setBatchSize(???).setLearningRate(???).setMaxEpoch(???)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# training\n",
    "\n",
    "print(\"training starting...\")\n",
    "## TODO  : start training by calling 'fit' method on 'df'\n",
    "nnModel = estimator.???(???)\n",
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "\n",
    "## TODO : create predictions by running 'transform' on 'df'\n",
    "results = nnModel.???(???)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Handling Images\n",
    "\n",
    "Analytics zoo has some tools to help us load image dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ./06-1-prep.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from zoo.pipeline.nnframes import NNImageReader\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "import re\n",
    "\n",
    "image_path = \"../../data/cat-dog/sample/train/*.jpg\"\n",
    "\n",
    "## TODO : Use 'NNImageReader.readImages' function\n",
    "##    - first argument is 'image_path'\n",
    "##    - second argument is SparkContext 'sc'\n",
    "imageDF = NNImageReader.readImages(???, ???)\n",
    "\n",
    "getName = udf(lambda row:\n",
    "                  re.search(r'(cat|dog)\\.([\\d]*)\\.jpg', row[0], re.IGNORECASE).group(0),\n",
    "                  StringType())\n",
    "\n",
    "## TODO : return 1.0 for cat 2.0 for dog\n",
    "getLabel = udf(lambda name: ??? if name.startswith('cat') else ???, DoubleType())\n",
    "\n",
    "labelDF = imageDF.withColumn(\"name\", getName(col(\"image\"))) \\\n",
    "        .withColumn(\"label\", getLabel(col('name')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : sample 10% (0.1) and display\n",
    "labelDF.sample(False, ???).select(\"name\",\"label\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO : split 80% training and 20% validation\n",
    "(trainingDF, validationDF) = labelDF.randomSplit([???, ???])\n",
    "\n",
    "\n",
    "## TODO : print training / validation stats\n",
    "print(\"training set count \", trainingDF.??? ())\n",
    "print(\"validation set count \", validationDF.??? ())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
